{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pure = '/kaggle/input/break224/break224/gcn_224/Train_gcn/*'\n",
    "#val_pure =  '/kaggle/input/break224/break224/gcn_224/Val_gcn/*'\n",
    "train_pure = 'gcn_224/Train_gcn/*'\n",
    "val_pure =  'gcn_224/Val_gcn/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "train_files = glob.glob(train_pure)\n",
    "val_files = glob.glob(val_pure)\n",
    "len(train_files)#6229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install barbar\n",
    "#!pip install swin-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import albumentations\n",
    "import albumentations.pytorch \n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from barbar import Bar\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "    torch.backends.cudnn.deterministic=False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_data(Dataset):\n",
    "    def __init__(self, data, transforms=None):\n",
    "        self.image_list = data\n",
    "        self.data_len = len(self.image_list)\n",
    "        self.transforms = transforms\n",
    "        self.eicls = [\"A\", \"F\", \"TA\", \"PT\", \"DC\", \"LC\", \"MC\", \"PC\"]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        current_image_path = self.image_list[index]\n",
    "        im_as_im = cv2.imread(current_image_path)\n",
    "        im_as_im = cv2.cvtColor(im_as_im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform label encoding for multi-label classification\n",
    "        parts = current_image_path.split('_')[-1].split('-')\n",
    "        if parts[2]==\"13412\":\n",
    "            labels =[0,0,0,0,1,1,0,0]\n",
    "        else:\n",
    "            labels = [int(label == parts[0]) for label in self.eicls]       \n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=im_as_im)\n",
    "            im_as_im = augmented['image']\n",
    "\n",
    "        return (im_as_im, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train': albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),     \n",
    "    albumentations.OneOf([\n",
    "                          albumentations.HorizontalFlip(),\n",
    "                          albumentations.RandomRotate90(),\n",
    "                          albumentations.VerticalFlip()            \n",
    "    ], p=0.75),\n",
    "    albumentations.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                       std=(0.229, 0.224, 0.225), p=1),\n",
    "    albumentations.pytorch.transforms.ToTensorV2()]),\n",
    "    \n",
    "    'valid': albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),     \n",
    "    albumentations.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                       std=(0.229, 0.224, 0.225), p=1),\n",
    "    albumentations.pytorch.transforms.ToTensorV2()]),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=My_data(train_files,transforms=transform['train'])\n",
    "valid=My_data(val_files,transforms=transform['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([0,0,0,0,0,0,0,0])\n",
    "for _,label in train:\n",
    "    a=a+label\n",
    "print(a)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.tensor([0,0,0,0,0,0,0,0])\n",
    "for _,label in valid:\n",
    "    b=b+label\n",
    "print(b)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class_samples = [367, 803, 456, 370, 2763, 492, 629, 449]  # Number of samples in each class\n",
    "\n",
    "total_samples = sum(class_samples)\n",
    "class_weights = [total_samples / (s + 1e-8) for s in class_samples]\n",
    "class_weights_sum = sum(class_weights)\n",
    "class_weights_normalized = [w / class_weights_sum for w in class_weights]\n",
    "class_weights_normalized = torch.tensor(class_weights_normalized)\n",
    "\n",
    "print(class_weights_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=train, batch_size=4,shuffle=True,num_workers=2,\n",
    "                                              pin_memory=True,prefetch_factor=2)\n",
    "valid_dataloader=  torch.utils.data.DataLoader(dataset=valid,batch_size=4,shuffle=False,num_workers=2,\n",
    "                                               pin_memory=True ,prefetch_factor=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in valid_dataloader:\n",
    "    print(i)\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.models import create_model\n",
    "from timm.data import create_transform\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define device\n",
    "# Define Swin Transformer v2 model\n",
    "model_name = 'swin_base_patch4_window7_224'\n",
    "num_classes = 8\n",
    "model = create_model(\n",
    "    model_name=model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=num_classes,\n",
    "    drop_rate=0.5,\n",
    "    drop_path_rate=0.2,\n",
    "    checkpoint_path=None\n",
    ")\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\"\"\"for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True  \"\"\" \n",
    "#model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "for param in model.layers.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.layers[3].parameters():\n",
    "    param.requires_grad = True    \n",
    "\n",
    "# Iterate over the parameters and check requires_grad\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter '{name}' requires grad.\")\n",
    "    else:\n",
    "        print(f\"Parameter '{name}' does not require grad.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/anaconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "model = timm.create_model(\n",
    "    'swinv2_small_window8_256.ms_in1k',\n",
    "    pretrained=True,\n",
    "    features_only=False,\n",
    "    num_classes = 8,drop_rate=0.5,\n",
    "    drop_path_rate=0.2, \n",
    "    checkpoint_path=None \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformerV2(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerV2Stage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (2): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (3): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (4): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.070)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.070)\n",
       "        )\n",
       "        (5): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (6): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (7): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (8): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.104)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.104)\n",
       "        )\n",
       "        (9): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.113)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.113)\n",
       "        )\n",
       "        (10): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.122)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.122)\n",
       "        )\n",
       "        (11): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.130)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.130)\n",
       "        )\n",
       "        (12): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.139)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.139)\n",
       "        )\n",
       "        (13): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.148)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.148)\n",
       "        )\n",
       "        (14): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.157)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.157)\n",
       "        )\n",
       "        (15): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.165)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.165)\n",
       "        )\n",
       "        (16): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.174)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.174)\n",
       "        )\n",
       "        (17): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.183)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.183)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.191)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.191)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.200)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.200)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=8, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[0].blocks[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block1.drop_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "block0 = model.layers[0].blocks [0]\n",
    "block1 = model.layers[0].blocks [1]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "\n",
    "class ParallelAttention (torch.nn.Module):\n",
    "    def __init__ (self, attention0, attention1):\n",
    "        super ().__init__ ()\n",
    "        self.attention0 = attention0\n",
    "        self.attention1 = attention1\n",
    "        self.layernorm1 = block1.norm1 # copy the layernorm_before module from block0\n",
    "        self.drop_path1 = block1.drop_path1 # copy the drop_path module from block0\n",
    "        self.mlp = block1.mlp # copy the intermediate module from block0\n",
    "        self.norm2 = block1.norm2 # copy the output module from block0\n",
    "        self.drop_path2 = block1.drop_path2\n",
    "    \n",
    "\n",
    "def forward (self, x):\n",
    "    x0 = self.attention0 (x) # apply attention module of block0\n",
    "    x1 = self.attention1 (x) # apply attention module of block1\n",
    "    x = torch.cat ([x0, x1], dim= -1) # concatenate along the last dimension\n",
    "    x = self.layernorm1 (x) # apply layernorm_before module\n",
    "    x = self.drop_path1 (x) # apply drop_path module\n",
    "    x = self.mlp (x) # apply intermediate module\n",
    "    x = self.drop_path2 (x) # apply output module\n",
    "    x=  self.norm2 (x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[0].blocks [0]\n",
    "block1 = model.layers[0].blocks [1]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block0=ParallelAttention(attention0,attention1)\n",
    "model.layers[0].blocks=new_block0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[1].blocks [0]\n",
    "block1 = model.layers[1].blocks [1]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block1=ParallelAttention(attention0,attention1)\n",
    "model.layers[1].blocks=new_block1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [0]\n",
    "block1 = model.layers[2].blocks [1]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block2=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[0]=new_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [2]\n",
    "block1 = model.layers[2].blocks [3]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block3=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[1]=new_block3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [4]\n",
    "block1 = model.layers[2].blocks [5]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block4=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[2]=new_block4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [6]\n",
    "block1 = model.layers[2].blocks [7]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block5=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[3]=new_block5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [8]\n",
    "block1 = model.layers[2].blocks [9]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block6=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[4]=new_block6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [10]\n",
    "block1 = model.layers[2].blocks [11]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block7=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[5]=new_block7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [12]\n",
    "block1 = model.layers[2].blocks [13]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block8=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[6]=new_block8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [14]\n",
    "block1 = model.layers[2].blocks [15]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block9=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[7]=new_block9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[2].blocks [16]\n",
    "block1 = model.layers[2].blocks [17]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block10=ParallelAttention(attention0,attention1)\n",
    "model.layers[2].blocks[7]=new_block10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "block0 = model.layers[3].blocks [0]\n",
    "block1 = model.layers[3].blocks [1]\n",
    "attention0=block0.attn\n",
    "attention1=block1.attn\n",
    "new_block11=ParallelAttention(attention0,attention1)\n",
    "model.layers[3].blocks=new_block11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17, 7, -1):\n",
    "    del model.layers[2].blocks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformerV2(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerV2Stage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): ParallelAttention(\n",
       "        (attention0): WindowAttention(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "          )\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (attention1): WindowAttention(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "          )\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path1): DropPath(drop_prob=0.009)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path2): DropPath(drop_prob=0.009)\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ParallelAttention(\n",
       "        (attention0): WindowAttention(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "          )\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (attention1): WindowAttention(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "          )\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path1): DropPath(drop_prob=0.026)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path2): DropPath(drop_prob=0.026)\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (1): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (2): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (3): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (4): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.113)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.113)\n",
       "        )\n",
       "        (5): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.130)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.130)\n",
       "        )\n",
       "        (6): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.148)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.148)\n",
       "        )\n",
       "        (7): ParallelAttention(\n",
       "          (attention0): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (attention1): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.183)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.183)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ParallelAttention(\n",
       "        (attention0): WindowAttention(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "          )\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (attention1): WindowAttention(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "          )\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path1): DropPath(drop_prob=0.200)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path2): DropPath(drop_prob=0.200)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=8, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train in this case for 10 epochs lr constraint on layer[3]\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.layers[3].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs=50\n",
    "for epoch in range(epochs):\n",
    "    time.sleep(1)\n",
    "    epoch=epoch+1\n",
    "    if epoch<=9:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layers[3].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "            if epoch== 9:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        print(name) \n",
    "                \n",
    "    elif  10<epoch<=19 :\n",
    "        for param in model.layers[2].parameters():\n",
    "            param.requires_grad = True\n",
    "    elif  20<epoch<=29:\n",
    "        for param in model.layers[1].parameters():\n",
    "            param.requires_grad = True\n",
    "    elif  30<epoch<=39:\n",
    "        for param in model.layers[0].parameters():\n",
    "            param.requires_grad = True\n",
    "    else: \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.patch_embed.parameters():\n",
    "    param.requires_grad = True    \n",
    "for param in model.layers[0].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.layers[1].parameters():\n",
    "    param.requires_grad = True   \n",
    "for param in model.layers[2].parameters():\n",
    "    param.requires_grad = True  \n",
    "for param in model.layers[3].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=model.layers[3].blocks[0].attn\n",
    "y=model.layers[3].blocks[1].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=model.layers[3].blocks[1].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ConcatenatedModule(nn.Module):\n",
    "    def __init__(self, layer1, layer2,layer3):\n",
    "        super(ConcatenatedModule, self).__init__()\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "        self.layer3 = layer3\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.layer1(x)\n",
    "        output2 = self.layer2(x)\n",
    "        output3 = torch.cat((output1, output2), dim=1)\n",
    "        \n",
    "        output= self.layer3(output3)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_module = ConcatenatedModule(x,y,z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=model.stages[-1].blocks[0]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FocalLossWithClassWeights(nn.Module):\n",
    "    def __init__(self, class_weights, alpha=1, gamma=2):\n",
    "        super(FocalLossWithClassWeights, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        class_weights = self.class_weights.to(target.device)\n",
    "        weighted_logits = class_weights * input\n",
    "        probs = self.sigmoid(weighted_logits)\n",
    "\n",
    "        loss = -(self.alpha * torch.pow(1 - probs, self.gamma) * target * torch.log(probs + 1e-8)\n",
    "                 + (1 - target) * torch.log(1 - probs + 1e-8))\n",
    "\n",
    "        return loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad =True\n",
    "    #print(param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#model = nn.DataParallel(model, device_ids = [0, 1])\n",
    "model = model.to(device)\n",
    "class_weights_normalized=class_weights_normalized.to(device)\n",
    "#criterion = torch.nn.BCEWithLogitsLoss(weight=class_weights_normalized)\n",
    "criterion = FocalLossWithClassWeights(class_weights_normalized)\n",
    "#criterion= torch.nn.MultiLabelSoftMarginLoss()\n",
    "#criterion =torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optims.Adahessian(model.parameters(),lr=0.001,hessian_power=0.5,weight_decay=0.1)\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "best_model_wts = model.state_dict()\n",
    "best_optimizer_state =optimizer.state_dict()\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, optimizer,scheduler, criterion):\n",
    "    #print('Training')\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    accum_iter = 4  \n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(Bar(dataloader)):\n",
    "            inputs = inputs.to(device)           \n",
    "            labels = labels.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #model.zero_grad(set_to_none=True)\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "            thresholds = [0.5, 0.5, 0.5,0.5,0.5,0.5,0.5,0.5]\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_running_loss += loss.item()* inputs.size(0)\n",
    "           # _ , preds = torch.max(outputs.data, 1)\n",
    "            # Apply sigmoid activation to obtain probabilities\n",
    "            #preds = (outputs > 0.5).float()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = torch.zeros_like(probs)\n",
    "            \n",
    "            # Set predicted labels based on the threshold\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                preds[:, i] = (probs[:, i] >= threshold).float()\n",
    "            train_running_correct += (preds == labels).all(dim=1).float().sum()\n",
    "            # Backpropagate the gradients\n",
    "            loss /= accum_iter\n",
    "            loss.backward() \n",
    "            \n",
    "                       \n",
    "            if ((i + 1) % accum_iter == 0) :\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "            \n",
    "    scheduler.step()\n",
    "            \n",
    "    train_loss = train_running_loss/len(dataloader.dataset)\n",
    "    train_accuracy = 100. * train_running_correct/len(dataloader.dataset)    \n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, optimizer, criterion):\n",
    "    #print('Validating')\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float()\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            thresholds = [0.5, 0.5, 0.5,0.5,0.5,0.5,0.5,0.5]\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()*inputs.size(0)\n",
    "            #_, preds = torch.max(outputs.data, 1)\n",
    "            #preds = (outputs > 0.5).float()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = torch.zeros_like(probs)\n",
    "            # Set predicted labels based on the threshold\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                preds[:, i] = (probs[:, i] >= threshold).float()\n",
    "            val_running_correct += (preds == labels).all(dim=1).float().sum()\n",
    "        \n",
    "    val_loss = val_running_loss/len(dataloader.dataset)\n",
    "    val_accuracy = 100. * val_running_correct/len(dataloader.dataset)        \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "history=[]\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#best_optimizer_state = copy.deepcopy(optimizer.state_dict())\n",
    "best_acc = 0.0\n",
    "epochs=50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    print('Epoch-{0}/{1} lr: {2}'.format(epoch+1,epochs ,optimizer.param_groups[0]['lr']))\n",
    "    if epoch<=9:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layers[3].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "                       \n",
    "    elif  10<epoch<=19 :\n",
    "        for param in model.layers[2].parameters():\n",
    "            param.requires_grad = True\n",
    "    elif  20<epoch<=29:\n",
    "        for param in model.layers[1].parameters():\n",
    "            param.requires_grad = True\n",
    "    elif  30<epoch<=39:\n",
    "        for param in model.layers[0].parameters():\n",
    "            param.requires_grad = True\n",
    "    else: \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    train_epoch_loss, train_epoch_accuracy = fit(model,train_dataloader,optimizer,scheduler,criterion)\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(model,valid_dataloader,optimizer,criterion)\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    history.append([epoch+1,train_epoch_loss, train_epoch_accuracy, val_epoch_loss, val_epoch_accuracy,(epoch_end-epoch_start)])\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f},Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f},time : {epoch_end-epoch_start:.2f}\")\n",
    "    torch.save({'history':history},'Master_his.pth')\n",
    "    if val_epoch_accuracy > best_acc:\n",
    "        best_acc = val_epoch_accuracy\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "       \n",
    "        best_epoch=epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': best_model_wts,\n",
    "            'loss': criterion,\n",
    "            'history':history,\n",
    "            'best_epoch': best_epoch+1,          \n",
    "    \n",
    "            }, 'Master.pth')    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, class_weights=None):\n",
    "        super(FocalLoss, self).__init__() \n",
    "        self.gamma = gamma \n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        probs = torch.sigmoid(logits) \n",
    "        ce_loss = nn.BCELoss()(probs, labels) \n",
    "        weight = torch.pow(1 - probs, self.gamma) \n",
    "        if self.class_weights is not None: \n",
    "            weight = weight * self.class_weights \n",
    "            loss = ce_loss * weight \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs=50\n",
    "for epoch in range(epochs):\n",
    "    time.sleep(1)\n",
    "    epoch=epoch+1\n",
    "    if epoch<=9:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False        \n",
    "        for param in model.norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True       \n",
    "    \n",
    "    else: \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "history=[]\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#best_optimizer_state = copy.deepcopy(optimizer.state_dict())\n",
    "best_acc = 0.0\n",
    "epochs=50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    print('Epoch-{0}/{1} lr: {2}'.format(epoch+1,epochs ,optimizer.param_groups[0]['lr']))\n",
    "    if epoch <= 9:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layers[3].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif 10 <= epoch <= 19:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layers[2].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.layers[3].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif 20 <= epoch <= 29:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layers[1].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.layers[2].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.layers[3].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif 30 <= epoch <= 39:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layers[0].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.layers[1].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.layers[2].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.layers[3].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        \n",
    "    train_epoch_loss, train_epoch_accuracy = fit(model,train_dataloader,optimizer,scheduler,criterion)\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(model,valid_dataloader,optimizer,criterion)\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    history.append([epoch+1,train_epoch_loss, train_epoch_accuracy, val_epoch_loss, val_epoch_accuracy,(epoch_end-epoch_start)])\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f},Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f},time : {epoch_end-epoch_start:.2f}\")\n",
    "    torch.save({'history':history},'Master_his.pth')\n",
    "    if val_epoch_accuracy > best_acc:\n",
    "        best_acc = val_epoch_accuracy\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "       \n",
    "        best_epoch=epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': best_model_wts,\n",
    "            'loss': criterion,\n",
    "            'history':history,\n",
    "            'best_epoch': best_epoch+1,          \n",
    "    \n",
    "            }, 'Master.pth')    \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
